- Agentic JWT: A Secure Delegation Protocol for Autonomous AI Agents https://arxiv.org/abs/2509.13597
- Who Taught the Lie? Responsibility Attribution for Poisoned Knowledge in Retrieval-Augmented Generation https://arxiv.org/abs/2509.13772
- Defending against Indirect Prompt Injection by Instruction Detection https://arxiv.org/abs/2505.06311
- Evaluating and Improving the Robustness of Security Attack Detectors Generated by LLMs https://arxiv.org/abs/2411.18216
- GuardianPWA: Enhancing Security Throughout the Progressive Web App Installation Lifecycle https://arxiv.org/abs/2509.13561
- Demystifying Progressive Web Application Permission Systems https://arxiv.org/abs/2509.13563
- AQUA-LLM: Evaluating Accuracy, Quantization, and Adversarial Robustness Trade-offs in LLMs for Cybersecurity Question Answering https://arxiv.org/abs/2509.13514
- Towards Trustworthy Agentic IoEV: AI Agents for Explainable Cyberthreat Mitigation and State Analytics https://arxiv.org/abs/2509.12233
- Secure Human Oversight of AI: Exploring the Attack Surface of Human Oversight https://arxiv.org/abs/2509.12290
- Amulet: a Python Library for Assessing Interactions Among ML Defenses and Risks https://arxiv.org/abs/2509.12386
- Yet Another Watermark for Large Language Models https://arxiv.org/abs/2509.12574
- Jailbreaking Large Language Models Through Content Concretization https://arxiv.org/abs/2509.12937
- xOffense: An AI-driven autonomous penetration testing framework with offensive knowledge-enhanced LLMs and multi agent systems https://arxiv.org/abs/2509.13021
- Redefining Website Fingerprinting Attacks With Multiagent LLMs https://arxiv.org/abs/2509.12462
- PromptSleuth: Detecting Prompt Injection via Semantic Intent Invariance https://arxiv.org/abs/2508.20890
- Your Compiler is Backdooring Your Model: Understanding and Exploiting Compilation Inconsistency Vulnerabilities in Deep Learning Compilers https://arxiv.org/abs/2509.11173
- Visual Contextual Attack: Jailbreaking MLLMs with Image-Driven Context Injection https://arxiv.org/abs/2507.02844
- AegisShield: Democratizing Cyber Threat Modeling with Generative AI https://arxiv.org/abs/2509.10482
- EchoLeak: The First Real-World Zero-Click Prompt Injection Exploit in a Production LLM System https://arxiv.org/abs/2509.10540
- The Coding Limits of Robust Watermarking for Generative Models https://arxiv.org/abs/2509.10577
- LLM in the Middle: A Systematic Review of Threats and Mitigations to Real-World LLM-based Systems https://arxiv.org/abs/2509.10682
- Large Language Models for Security Operations Centers: A Comprehensive Survey https://arxiv.org/abs/2509.10858
- From Firewalls to Frontiers: AI Red-Teaming is a Domain-Specific Evolution of Cyber Red-Teaming https://arxiv.org/abs/2509.11398
- Exploit Tool Invocation Prompt for Tool Behavior Hijacking in LLM-Based Agentic System https://arxiv.org/abs/2509.05755
- Multi-Agent Systems Execute Arbitrary Malicious Code https://arxiv.org/abs/2503.12188
- Enhancing Prompt Injection Attacks to LLMs via Poisoning Alignment https://arxiv.org/abs/2410.14827
- Can LLM Prompting Serve as a Proxy for Static Analysis in Vulnerability Detection https://arxiv.org/abs/2412.12039
- When Your Reviewer is an LLM: Biases, Divergence, and Prompt Injection Risks in Peer Review https://arxiv.org/abs/2509.09912
- Character-Level Perturbations Disrupt LLM Watermarks https://arxiv.org/abs/2509.09112
- What You Code Is What We Prove: Translating BLE App Logic into Formal Models with LLMs for Vulnerability Detection https://arxiv.org/abs/2509.09291
- Prompt Pirates Need a Map: Stealing Seeds helps Stealing Prompts https://arxiv.org/abs/2509.09488
- ORCA: Unveiling Obscure Containers In The Wild https://arxiv.org/abs/2509.09322
- I Know Who Clones Your Code: Interpretable Smart Contract Similarity Detection https://arxiv.org/abs/2509.09630
- Information Inference Diagrams: Complementing Privacy and Security Analyses Beyond Data Flows https://arxiv.org/abs/2405.08356
- Can LLMs Hack Enterprise Networks? Autonomous Assumed Breach Penetration-Testing Active Directory Networks https://arxiv.org/abs/2502.04227
- ACE: A Security Architecture for LLM-Integrated App Systems https://arxiv.org/abs/2504.20984
- Establishing a Baseline of Software Supply Chain Security Task Adoption by Software Organizations https://arxiv.org/abs/2509.08083
- Accelerating AI Development with Cyber Arenas https://arxiv.org/abs/2509.08200
- Architecting Resilient LLM Agents: A Guide to Secure Plan-then-Execute Implementations https://arxiv.org/abs/2509.08646
- Adversarial Attacks Against Automated Fact-Checking: A Survey https://arxiv.org/abs/2509.08463
- CyberRAG: An Agentic RAG cyber attack classification and reporting tool https://arxiv.org/abs/2507.02424
- Backdoor Attacks and Defenses in Computer Vision Domain: A Survey https://arxiv.org/abs/2509.07504
- The Signalgate Case is Waiving a Red Flag to All Organizational and Behavioral Cybersecurity Leaders, Practitioners, and Researchers: Are We Receiving the Signal Amidst the Noise? https://arxiv.org/abs/2509.07053
- Guided Reasoning in LLM-Driven Penetration Testing Using Structured Attack Trees https://arxiv.org/abs/2509.07939
- Personalized Attacks of Social Engineering in Multi-turn Conversations: LLM Agents for Simulation and Detection https://arxiv.org/abs/2503.15552
- Factuality Beyond Coherence: Evaluating LLM Watermarking Methods for Medical Texts https://arxiv.org/abs/2509.07755
- Between a Rock and a Hard Place: Exploiting Ethical Reasoning to Jailbreak LLMs https://arxiv.org/abs/2509.05367
- Behind the Mask: Benchmarking Camouflaged Jailbreaks in Large Language Models https://arxiv.org/abs/2509.05471
- ThreatGPT: An Agentic AI Framework for Enhancing Public Safety through Threat Modeling https://arxiv.org/abs/2509.05379
- Reasoning Introduces New Poisoning Attacks Yet Makes Them More Complicated https://arxiv.org/abs/2509.05739
- Exploit Tool Invocation Prompt for Tool Behavior Hijacking in LLM-Based Agentic System https://arxiv.org/abs/2509.05755
- Decoding Latent Attack Surfaces in LLMs: Prompt Injection via HTML in Web Summarization https://arxiv.org/abs/2509.05831
- Multimodal Prompt Injection Attacks: Risks and Defenses for Modern LLMs https://arxiv.org/abs/2509.05883
- Dataset Ownership in the Era of Large Language Models https://arxiv.org/abs/2509.05921
- Measuring the Vulnerability Disclosure Policies of AI Vendors https://arxiv.org/abs/2509.06136
- Mind Your Server: A Systematic Study of Parasitic Toolchain Attacks on the MCP Ecosystem https://arxiv.org/abs/2509.06572
- LLMs in Cybersecurity: Friend or Foe in the Human Decision Loop? https://arxiv.org/abs/2509.06595
- Neuro-Symbolic AI for Cybersecurity: State of the Art, Challenges, and Opportunities https://arxiv.org/abs/2509.06921
- Six Million (Suspected) Fake Stars in GitHub: A Growing Spiral of Popularity Contests, Spams, and Malware https://arxiv.org/abs/2412.13459
- The Information Security Awareness of Large Language Models https://arxiv.org/abs/2411.13207
- Breaking to Build: A Threat Model of Prompt-Based Attacks for Securing LLMs https://arxiv.org/abs/2509.04615
- AgentArmor: Enforcing Program Analysis on Agent Runtime Trace to Defend Against Prompt Injection https://arxiv.org/abs/2508.01249
- Poisoned at Scale: A Scalable Audit Uncovers Hidden Scam Endpoints in Production LLMs https://arxiv.org/abs/2509.02372
- Probabilistic Modeling of Jailbreak on Multimodal LLMs: From Quantification to Application https://arxiv.org/abs/2503.06989
- Prompt-in-Content Attacks: Exploiting Uploaded Inputs to Hijack LLM Behavior https://arxiv.org/abs/2508.19287
- POT: Inducing Overthinking in LLMs via Black-Box Iterative Optimization https://arxiv.org/abs/2508.19277
- A Systematic Approach to Predict the Impact of Cybersecurity Vulnerabilities Using LLMs https://arxiv.org/abs/2508.18439
- Generative Artificial Intelligence-Supported Pentesting: A Comparison between Claude Opus, GPT-4, and Copilot https://arxiv.org/abs/2501.06963
- Trust Me, I Know This Function: Hijacking LLM Static Analysis using Bias https://arxiv.org/abs/2508.17361
- Defending Against Prompt Injection With a Few DefensiveTokens https://arxiv.org/abs/2507.07974
- Security Concerns for Large Language Models: A Survey https://arxiv.org/abs/2505.18889
- Mitigating Jailbreaks with Intent-Aware LLMs https://arxiv.org/abs/2508.12072
- Fortifying the Agentic Web: A Unified Zero-Trust Architecture Against Logic-layer Threats https://arxiv.org/abs/2508.12259
- Vulnerabilities in AI-generated Image Detection: The Challenge of Adversarial Attacks https://arxiv.org/abs/2407.20836
- A Whole New World: Creating a Parallel-Poisoned Web Only AI-Agents Can See https://arxiv.org/abs/2509.00124
- Web Fraud Attacks Against LLM-Driven Multi-Agent Systems https://arxiv.org/abs/2509.01211
- Throttling Web Agents Using Reasoning Gates https://arxiv.org/abs/2509.01619
- When Memory Becomes a Vulnerability: Towards Multi-turn Jailbreak Attacks against Text-to-Image Generation Systems https://arxiv.org/abs/2504.20376
- Publish to Perish: Prompt Injection Attacks on LLM-Assisted Peer Review https://arxiv.org/abs/2508.20863
- Servant, Stalker, Predator: How An Honest, Helpful, And Harmless (3H) Agent Unlocks Adversarial Skills https://arxiv.org/abs/2508.19500
- When AIOps Become "AI Oops": Subverting LLM-driven IT Operations via Telemetry Manipulation https://arxiv.org/abs/2508.06394
- A Systematic Survey of Model Extraction Attacks and Defenses: State-of-the-Art and Perspectives https://arxiv.org/abs/2508.15031
- MCP-Guard: A Defense Framework for Model Context Protocol Integrity in Large Language Model Applications https://arxiv.org/abs/2508.10991
- PromptKeeper: Safeguarding System Prompts for LLMs https://arxiv.org/abs/2412.13426
- MCPTox: A Benchmark for Tool Poisoning Attack on Real-World MCP Servers https://arxiv.org/abs/2508.14925
- Security Steerability is All You Need https://arxiv.org/abs/2504.19521
- Involuntary Jailbreak https://arxiv.org/abs/2508.13246
- PII Jailbreaking in LLMs via Activation Steering Reveals Personal Information Leakage https://arxiv.org/abs/2507.02332
- Can AI Keep a Secret? Contextual Integrity Verification: A Provable Security Architecture for LLMs https://arxiv.org/abs/2508.09288
- From CVE Entries to Verifiable Exploits: An Automated Multi-Agent Framework for Reproducing CVEs https://arxiv.org/abs/2509.01835
- From Attack Descriptions to Vulnerabilities: A Sentence Transformer-Based Approach https://arxiv.org/abs/2509.02077
- From Promise to Peril: Rethinking Cybersecurity Red and Blue Teaming in the Age of LLMs https://arxiv.org/abs/2506.13434
- AgentVigil: Generic Black-Box Red-teaming for Indirect Prompt Injection against LLM Agents https://arxiv.org/abs/2505.05849
- Benchmarking Practices in LLM-driven Offensive Security: Testbeds, Metrics, and Experiment Design https://arxiv.org/abs/2504.10112
- PiCo: Jailbreaking Multimodal Large Language Models via $\textbf{Pi}$ctorial $\textbf{Co}$de Contextualization https://arxiv.org/abs/2504.01444
- Effective Red-Teaming of Policy-Adherent Agents https://arxiv.org/abs/2506.09600
- PRISON: Unmasking the Criminal Potential of Large Language Models https://arxiv.org/abs/2506.16150
- DrunkAgent: Stealthy Memory Corruption in LLM-Powered Recommender Agents https://arxiv.org/abs/2503.23804
- SafeGenBench: A Benchmark Framework for Security Vulnerability Detection in LLM-Generated Code https://arxiv.org/abs/2506.05692
- From LLMs to MLLMs to Agents: A Survey of Emerging Paradigms in Jailbreak Attacks and Defenses within LLM Ecosystem https://arxiv.org/abs/2506.15170
- Organizational Adaptation to Generative AI in Cybersecurity: A Systematic Review https://arxiv.org/abs/2506.12060
- Context manipulation attacks : Web agents are susceptible to corrupted memory https://arxiv.org/abs/2506.17318
- SAVANT: Vulnerability Detection in Application Dependencies through Semantic-Guided Reachability Analysis https://arxiv.org/abs/2506.17798
- AgentAlign: Navigating Safety Alignment in the Shift from Informative to Agentic Large Language Models https://arxiv.org/abs/2505.23020
- MCP Safety Training: Learning to Refuse Falsely Benign MCP Exploits using Improved Preference Alignment https://arxiv.org/abs/2505.23634
- Artemis: Toward Accurate Detection of Server-Side Request Forgeries through LLM-Assisted Inter-Procedural Path-Sensitive Taint Analysis https://arxiv.org/abs/2502.21026
- Hijacking Large Language Models via Adversarial In-Context Learning https://arxiv.org/abs/2311.09948
- Beyond Surface-Level Patterns: An Essence-Driven Defense Framework Against Jailbreak Attacks in LLMs https://arxiv.org/abs/2502.19041
- IRCopilot: Automated Incident Response with Large Language Models https://arxiv.org/abs/2505.20945
- Red-Teaming Text-to-Image Systems by Rule-based Preference Modeling https://arxiv.org/abs/2505.21074
- InjectLab: A Tactical Framework for Adversarial Threat Modeling Against Large Language Models https://arxiv.org/abs/2505.18156
- GenAI Security: Outsmarting the Bots with a Proactive Testing Framework https://arxiv.org/abs/2505.18172
- Beyond the Protocol: Unveiling Attack Vectors in the Model Context Protocol Ecosystem https://arxiv.org/abs/2506.02040
- Developing a Risk Identification Framework for Foundation Model Uses https://arxiv.org/abs/2506.02066
- Docker under Siege: Securing Containers in the Modern Era https://arxiv.org/abs/2506.02043
- MISLEADER: Defending against Model Extraction with Ensembles of Distilled Models https://arxiv.org/abs/2506.02362
- BitBypass: A New Direction in Jailbreaking Aligned Large Language Models with Bitstream Camouflage https://arxiv.org/abs/2506.02479
- A Review of Various Datasets for Machine Learning Algorithm-Based Intrusion Detection System: Advances and Challenges https://arxiv.org/abs/2506.02438
- Attention Knows Whom to Trust: Attention-based Trust Management for LLM Multi-Agent Systems https://arxiv.org/abs/2506.02546
- CyberGym: Evaluating AI Agents' Cybersecurity Capabilities with Real-World Vulnerabilities at Scale https://arxiv.org/abs/2506.02548
- AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs https://arxiv.org/abs/2404.16873
- VPI-Bench: Visual Prompt Injection Attacks for Computer-Use Agents https://arxiv.org/abs/2506.02456
- Chain-of-Jailbreak Attack for Image Generation Models via Editing Step by Step https://arxiv.org/abs/2410.03869
- Adversarial Inception Backdoor Attacks against Reinforcement Learning https://arxiv.org/abs/2410.13995
- Unveiling Privacy Risks in LLM Agent Memory https://arxiv.org/abs/2502.13172
- When GPT Spills the Tea: Comprehensive Assessment of Knowledge File Leakage in GPTs https://arxiv.org/abs/2506.00197
- Hush! Protecting Secrets During Model Training: An Indistinguishability Approach https://arxiv.org/abs/2506.00201
- SATA: A Paradigm for LLM Jailbreak via Simple Assistive Task Linkage https://arxiv.org/abs/2412.15289
- Adversarial Threat Vectors and Risk Mitigation for Retrieval-Augmented Generation Systems https://arxiv.org/abs/2506.00281
- Teaching an Old LLM Secure Coding: Localized Preference Optimization on Distilled Preferences https://arxiv.org/abs/2506.00419
- Simple Prompt Injection Attacks Can Leak Personal Data Observed by LLM Agents During Task Execution https://arxiv.org/abs/2506.01055
- SPEAR: Security Posture Evaluation using AI Planner-Reasoning on Attack-Connectivity Hypergraphs https://arxiv.org/abs/2506.01227
- Align is not Enough: Multimodal Universal Jailbreak Attack against Multimodal Large Language Models https://arxiv.org/abs/2506.01307
- Which Factors Make Code LLMs More Vulnerable to Backdoor Attacks? A Systematic Study https://arxiv.org/abs/2506.01825
- Data Poisoning for In-context Learning https://arxiv.org/abs/2402.02160
- Safety at Scale: A Comprehensive Survey of Large Model Safety https://arxiv.org/abs/2502.05206
- Primus: A Pioneering Collection of Open-Source Datasets for Cybersecurity LLM Training https://arxiv.org/abs/2502.11191
- Security Concerns for Large Language Models: A Survey https://arxiv.org/abs/2505.18889
- RenderBender: A Survey on Adversarial Attacks Using Differentiable Rendering https://arxiv.org/abs/2411.09749
- Are We in the AI-Generated Text World Already? Quantifying and Monitoring AIGT on Social Media https://arxiv.org/abs/2412.18148
- A Unified Framework for Human AI Collaboration in Security Operations Centers with Trusted Autonomy https://arxiv.org/abs/2505.23397
- Zero-Trust Foundation Models: A New Paradigm for Secure and Collaborative Artificial Intelligence for Internet of Things https://arxiv.org/abs/2505.23792
- System Prompt Extraction Attacks and Defenses in Large Language Models https://arxiv.org/abs/2505.23817
- LLM Agents Should Employ Security Principles https://arxiv.org/abs/2505.24019
- PentestAgent: Incorporating LLM Agents to Automated Penetration Testing https://arxiv.org/abs/2411.05185
- Permissioned LLMs: Enforcing Access Control in Large Language Models https://arxiv.org/abs/2505.22860
- Securing AI Agents with Information-Flow Control https://arxiv.org/abs/2505.23643
- A Human Study of Cognitive Biases in Web Application Security https://arxiv.org/abs/2505.12018
